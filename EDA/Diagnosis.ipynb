{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T18:14:00.529565Z",
     "start_time": "2021-03-09T18:14:00.525548Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import os \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T18:14:41.195002Z",
     "start_time": "2021-03-09T18:14:41.184030Z"
    }
   },
   "outputs": [],
   "source": [
    "class Train_Diagnosis():\n",
    "    '''\n",
    "    This class is actually used to train and update the model and the data that I have \n",
    "    Prediction is going to be done through a function.\n",
    "    \n",
    "    Class to predict the ailment based on the textual information provided \n",
    "    on the site \n",
    "    ATTRIBUTES:\n",
    "    data : the processed and lemmatized dataframe loaded from the memory (\n",
    "    contains stemmed_data column and the prompt column)\n",
    "    ailments_dict_keyname/_keyint : dictionaries representing the unique ailments present \n",
    "                   in the data set \n",
    "    vectorizer : TfIdf vectorizer used for transforming data \n",
    "    ( based on the latest state of the data file)\n",
    "    model : \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.data = self.get_latest_data()\n",
    "        self.ailments_dict_keyname = self.get_ailments(0)\n",
    "        self.ailments_dict_keyint = self.get_ailments(1)\n",
    "        self.vectorizer = self.get_vectorizer()\n",
    "        self.model = self.get_model()\n",
    "        self.vector_path = 'models/vectorizer.pkl'\n",
    "        self.model_path = 'models/model.pkl'\n",
    "        \n",
    "    def get_latest_data(self):\n",
    "        '''Function to load in the latest dataframe \n",
    "        that you have for the model training '''\n",
    "        data = pd.read_csv(r'data/trial_data.csv')\n",
    "        return data\n",
    "    \n",
    "    def get_ailments(self,type_of_dict):\n",
    "        '''Function to load the unique ailment dictionary\n",
    "        PARAMETERS: type_of_dict: 0,1 : how to form the \n",
    "                     keys of the dictionary \n",
    "        RETURNS : dictionary of the ailments'''\n",
    "        D = {}\n",
    "        ailments = self.data['Prompt'].unique()\n",
    "        if(type_of_dict == 0):\n",
    "            # By name\n",
    "            for i,k in enumerate(ailments):\n",
    "                D[k] = i\n",
    "        else:\n",
    "            # By indexing\n",
    "            for i,k in enumerate(ailments):\n",
    "                D[i] = k\n",
    "        return D\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        '''Return a vectorizer to fit on the data '''\n",
    "        TfIdf = TfidfVectorizer(stop_words = 'english', ngram_range= (1,3),max_df= 0.7)\n",
    "        return TfIdf\n",
    "    \n",
    "    def get_training_x(self):\n",
    "        '''Returns the transformed data for training'''\n",
    "        X = (self.vectorizer).fit_transform(self.data['stemmed_phrase'])\n",
    "        X = X.toarray()\n",
    "        # update the vectorizer here \n",
    "        self.vectorizer = self.vectorizer.fit(self.data['stemmed_phrase'])\n",
    "        # save the vectorizer at this point, after you have fit it \n",
    "        pickle.dump(self.vectorizer,open(self.vector_path,'wb'))\n",
    "        return X \n",
    "    \n",
    "    def get_training_y(self):\n",
    "        '''Returns the encoded classes for training'''\n",
    "        Y = self.data['Prompt'].map(self.ailments_dict_keyname)\n",
    "        return Y\n",
    "    \n",
    "    def get_model(self):\n",
    "        '''Returns a model for the data '''\n",
    "        M = RandomForestClassifier(n_estimators=36,min_samples_leaf=2)\n",
    "        return M        \n",
    "    # METHODS \n",
    "    # 1. Trains the model \n",
    "    \n",
    "    def train_model(self):\n",
    "        '''Trains the model as and when you want \n",
    "        with the loaded data'''\n",
    "        X = self.get_training_x()\n",
    "        Y = self.get_training_y()\n",
    "        # validation is actually done on the query not the test data\n",
    "        self.model.fit(X,Y)\n",
    "        pickle.dump(self.model,open(self.model_path,'wb'))\n",
    "\n",
    "        # that's it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T18:15:44.433797Z",
     "start_time": "2021-03-09T18:15:44.423825Z"
    }
   },
   "outputs": [],
   "source": [
    "class Predictions():\n",
    "    '''class to make the predictions given the model and then \n",
    "    append the query to the data set that you currently have '''\n",
    "    def __init__(self,model,data_path):\n",
    "        self.model = model\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        punctuation='[\"\\'?,\\.]'\n",
    "        self.abbr_dict = {\n",
    "            \"what's\":\"what is\",\n",
    "            \"what're\":\"what are\",\n",
    "            \"where's\":\"where is\",\n",
    "            \"where're\":\"where are\",\n",
    "            \"i'm\":\"i am\",\n",
    "            \"we're\":\"we are\",\n",
    "            \"it's\":\"it is\",\n",
    "            \"that's\":\"that is\",\n",
    "            \"there's\":\"there is\",\n",
    "            \"there're\":\"there are\",\n",
    "            \"i've\":\"i have\",\n",
    "            \"who've\":\"who have\",\n",
    "            \"would've\":\"would have\",\n",
    "            \"not've\":\"not have\",\n",
    "            \"i'll\":\"i will\",\n",
    "            \"it'll\":\"it will\",\n",
    "            \"isn't\":\"is not\",\n",
    "            \"wasn't\":\"was not\",\n",
    "            \"aren't\":\"are not\",\n",
    "            \"weren't\":\"were not\",\n",
    "            \"can't\":\"can not\",\n",
    "            \"couldn't\":\"could not\",\n",
    "            \"don't\":\"do not\",\n",
    "            \"didn't\":\"did not\",\n",
    "            \"shouldn't\":\"should not\",\n",
    "            \"wouldn't\":\"would not\",\n",
    "            \"doesn't\":\"does not\",\n",
    "            \"haven't\":\"have not\",\n",
    "            \"hasn't\":\"has not\",\n",
    "            \"hadn't\":\"had not\",\n",
    "            \"won't\":\"will not\",\n",
    "            punctuation:'',\n",
    "            '\\s+':' ', # replace multi space with one single space\n",
    "        }\n",
    "    def process_query(self,query):\n",
    "        \n",
    "        '''Returns a processed and stemmed query'''\n",
    "        query = query.lower()\n",
    "        res = ''\n",
    "        for k in query.split():\n",
    "            if k in self.abbr_dict:\n",
    "                res+=' ' + self.abbr_dict[k]\n",
    "            else:\n",
    "                res+=' ' + k \n",
    "        \n",
    "        res = ' '.join([self.stemmer.stem(y) for y in res.split()])\n",
    "        return res \n",
    "    \n",
    "    def append_query(self,query,ailment):\n",
    "        '''Take the query and prediction and then append it to original data '''\n",
    "        \n",
    "        col1 = 'stemmed_phrase'\n",
    "        col2 = 'Phrase'\n",
    "        self.data.append([{col1 : query , col2 : ailment}] , ignore_index = True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that the whole dataset does not need to be stemmed again and again\n",
    "- I will store the stemmed data and then as and when I get a query I would append the stemmed query at the end of the data set after each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T18:14:45.366209Z",
     "start_time": "2021-03-09T18:14:45.020645Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Train_Diagnosis()\n",
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T18:15:47.160827Z",
     "start_time": "2021-03-09T18:15:47.121994Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'punctuation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-83bf63382859>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain_Diagnosis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mailments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_ailments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mdiagnoser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPredictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'data/trial_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# parameter 1 ->\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# processes the query given by the site and make prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-e0539050a6df>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, data_path)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;34m\"hadn't\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"had not\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;34m\"won't\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"will not\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mpunctuation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;34m'\\s+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# replace multi space with one single space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'punctuation' is not defined"
     ]
    }
   ],
   "source": [
    "# get predictions needs to be a separate function as it just needs to get the \n",
    "# predictions \n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS \n",
    "\n",
    "## 1. prediction \n",
    "## moreover, model and vectorizer need not be loaded again and again\n",
    "\n",
    "# build 1 end point\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "\n",
    "def load_from_pickle(file):\n",
    "    loaded = pickle.load(open(file,'rb'))\n",
    "    return loaded\n",
    "\n",
    "# got the models     \n",
    "vectorizer = load_from_pickle('models/vectorizer.pkl')\n",
    "model = load_from_pickle('models/model.pkl')\n",
    "\n",
    "trainer = Train_Diagnosis()\n",
    "ailments = trainer.get_ailments(1)\n",
    "diagnoser = Predictions(model,'data/trial_data.csv')\n",
    "# parameter 1 -> \n",
    "    # processes the query given by the site and make prediction   \n",
    "    \n",
    "@app.route('/process',methods = ['GET'])\n",
    "def get_diagnosis():\n",
    "    q = request.args.get('query')\n",
    "    processed = diagnoser.process_query(q)\n",
    "    \n",
    "    # now transform\n",
    "    query = [processed]\n",
    "    query = vectorizer.transform(query)\n",
    "    \n",
    "    # and predict \n",
    "    preds = model.predict_proba(query)\n",
    "    res = list(np.argsort(preds))[0]\n",
    "    res = res[::-1][:3] # top 3 \n",
    "    ailment_top = ailments[res[0]]\n",
    "    \n",
    "    # append record to the data\n",
    "    diagnoser.append_query(query,ailment_top)\n",
    "    \n",
    "    #gather predictions \n",
    "    predictions = []\n",
    "    for k in res: \n",
    "        predictions.append(ailments[k])\n",
    "\n",
    "# parameter point 2 -> \n",
    "    # re-trains the model on the acquired data and \n",
    "    # reloads model and vectorizer \n",
    "    \n",
    "    # 0 -> do not train \n",
    "    # 1 -> train again \n",
    "    train = int(request.args.get('train')) # ->\n",
    "    if(train is not None and train == 1):\n",
    "        # means I need to train along with this query \n",
    "        trainer.train_model()\n",
    "        #at this point load the vectorizer and model again with the new queries\n",
    "        vectorizer = load_from_pickle('models/vectorizer.pkl')\n",
    "        model = load_from_pickle('models/model.pkl')\n",
    "    \n",
    "    return jsonify(predictions)\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run(port = 5000, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
